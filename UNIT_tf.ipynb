{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Image-image Translation(UNIT) TensorFlow Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/praveen/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funciton to chose the activation function\n",
    "def activate(linear, activation='leaky_relu'):\n",
    "        if activation == 'sigmoid':\n",
    "            return tf.nn.sigmoid(linear)\n",
    "        elif activation == 'softmax':\n",
    "            return tf.nn.softmax(linear)\n",
    "        elif activation == 'tanh':\n",
    "            return tf.nn.tanh(linear)\n",
    "        elif activation == 'relu':\n",
    "            return tf.nn.relu(linear)\n",
    "        elif activation == 'leaky_relu':\n",
    "            return tf.nn.leaky_relu(linear)\n",
    "        elif activation == 'linear':\n",
    "            return linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_noise_layer(input_layer, std=1.0):\n",
    "    noise = tf.random_normal(shape=tf.shape(input_layer), mean=0.0, stddev=std, dtype=tf.float32) \n",
    "    return tf.add(input_layer, noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_normalization_layer(input_layer):\n",
    "    \n",
    "    dimension = input_layer.get_shape().as_list()[-1]\n",
    "    \n",
    "    mean, variance = tf.nn.moments(input_layer, axes=[0, 1, 2])\n",
    "    \n",
    "    beta = tf.Variable(tf.constant(0.0, shape=[dimension]))\n",
    "    \n",
    "    gamma = tf.Variable(tf.constant(1.0, shape=[dimension]))\n",
    "\n",
    "    bn_layer = tf.nn.batch_normalization(input_layer, mean, variance, beta, gamma, variance_epsilon=1e-10)\n",
    "\n",
    "    return bn_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conv_layer(input_layer,     # The previous layer.\n",
    "                   filter_size,        # Width and height of each filter.\n",
    "                   num_filters,        # Number of filters.\n",
    "                   use_pooling=False,  \n",
    "                   pad = [],\n",
    "                   strides=[1,1,1,1],\n",
    "                   deconv=False,\n",
    "                   out_shape = [],      # Output shape in case of deconv \n",
    "                   batch_normalization=False,\n",
    "                   activation='leaky_relu'): # Use 2x2 max-pooling.\n",
    "\n",
    "    \n",
    "    num_input_channels = input_layer.get_shape().as_list()[-1]\n",
    "    # Shape of the filter-weights for the convolution. \n",
    "    # This format is determined by the TensorFlow API.\n",
    "    if deconv:\n",
    "        shape = [filter_size, filter_size, num_filters, num_input_channels]\n",
    "    else:\n",
    "        shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
    "\n",
    "    # Create new weights aka. filters with the given shape.\n",
    "    weights = tf.Variable(tf.truncated_normal(shape, stddev=0.05))\n",
    "\n",
    "    # Create new biases, one for each filter.\n",
    "    biases = tf.Variable(tf.constant(1.0, shape=[num_filters]))\n",
    "\n",
    "    if len(pad) > 0:\n",
    "        input_layer = tf.pad(input_layer, [[0, 0], [pad[0], pad[0]], [pad[1], pad[1]], [0, 0]], \"CONSTANT\")\n",
    "    else:\n",
    "        pad = [0,0]\n",
    "    # Create the TensorFlow operation for de-convolution.\n",
    "    if deconv:\n",
    "        #in_shape = input_layer.get_shape().as_list()\n",
    "        in_shape = tf.shape(input_layer)\n",
    "    \n",
    "        out_h = ((in_shape[1] - 1) * strides[1]) + filter_size - 2 * pad[0]\n",
    "        \n",
    "        out_w = ((in_shape[2] - 1) * strides[2]) + filter_size - 2 * pad[1]\n",
    "        \n",
    "        output_shape = tf.stack([in_shape[0], out_h, out_w, num_filters])\n",
    "        \n",
    "        layer = tf.nn.conv2d_transpose(value=input_layer, \n",
    "                                       filter=weights, \n",
    "                                       output_shape=output_shape, \n",
    "                                       strides=strides, \n",
    "                                       padding='SAME')\n",
    "        \n",
    "    else:\n",
    "        layer = tf.nn.conv2d(input=input_layer,\n",
    "                         filter=weights,\n",
    "                         strides=strides,\n",
    "                         padding='SAME')\n",
    "\n",
    "    # Add the biases to the results of the convolution.\n",
    "    # A bias-value is added to each filter-channel.\n",
    "    layer = tf.add(layer,biases)\n",
    "\n",
    "    # Use pooling to down-sample the image resolution?\n",
    "    if use_pooling:\n",
    "        # This is 2x2 max-pooling, which means that we\n",
    "        # consider 2x2 windows and select the largest value\n",
    "        # in each window. Then we move 2 pixels to the next window.\n",
    "        layer = tf.nn.max_pool(value=layer,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME')\n",
    "\n",
    "    # Batch normalization\n",
    "    if batch_normalization:\n",
    "        layer = batch_normalization_layer(layer)\n",
    "    \n",
    "    # Activation of the layers (ReLU).\n",
    "    layer = activate(layer, activation=activation)\n",
    "\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_res_block(X, filter_size=3, num_filters=3):\n",
    "    \n",
    "    num_filters = X.get_shape().as_list()[-1]\n",
    "    \n",
    "    layer_conv1 = create_conv_layer(input_layer=X,\n",
    "                                   filter_size=filter_size,\n",
    "                                   num_filters=num_filters,\n",
    "                                   use_pooling=False,\n",
    "                                   batch_normalization=True,\n",
    "                                   activation='relu')\n",
    "    \n",
    "    layer_conv2 = create_conv_layer(input_layer=layer_conv1,\n",
    "                                   filter_size=filter_size,\n",
    "                                   num_filters=num_filters,\n",
    "                                   use_pooling=False,\n",
    "                                   batch_normalization=False,\n",
    "                                   activation='linear')\n",
    "    \n",
    "    layer_conv2 = batch_normalization_layer(layer_conv2)\n",
    "    \n",
    "    layer_conv2 += X\n",
    "    \n",
    "    layer_res = activate(layer_conv2, activation='relu')\n",
    "    \n",
    "    return layer_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoder(X, layer_n, res_block_n):\n",
    "    \n",
    "    num_filters = X.get_shape().as_list()[-1]\n",
    "    \n",
    "    encoder = OrderedDict()\n",
    "    \n",
    "    encoder['layer_conv1'] = create_conv_layer(input_layer=X,\n",
    "                                       num_filters=num_filters,\n",
    "                                       filter_size=7,\n",
    "                                       strides=[1,1,1,1])\n",
    "    \n",
    "    for i in range(1, layer_n):\n",
    "        \n",
    "        encoder['layer_conv'+str(i+1)] = create_conv_layer(encoder[next(reversed(encoder))],\n",
    "                                       num_filters=num_filters*2,\n",
    "                                       filter_size=3,\n",
    "                                       strides=[1,2,2,1])\n",
    "        \n",
    "        num_filters = num_filters*2\n",
    "        \n",
    "    for i in range(0, res_block_n):\n",
    "        encoder['block_en_res'+str(i+1)] = create_res_block(encoder[next(reversed(encoder))],\n",
    "                                                           num_filters=num_filters)\n",
    "        \n",
    "    return encoder#[next(reversed(encoder))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_shared_layers(X, block_shared_n):\n",
    "    encoder_shared = OrderedDict()\n",
    "    decoder_shared = OrderedDict()\n",
    "    \n",
    "    encoder_shared_ = create_res_block(X)\n",
    "    encoder_shared['block_shared_res1'] = gaussian_noise_layer(encoder_shared_)\n",
    "    \n",
    "    for i in range(1, block_shared_n+1):\n",
    "        encoder_shared_ = create_res_block(encoder_shared[next(reversed(encoder_shared))])\n",
    "        encoder_shared['block_shared_res'+str(i+1)] = gaussian_noise_layer(encoder_shared_)\n",
    "    \n",
    "    decoder_shared['block_shared_res1'] = create_res_block(encoder_shared[next(reversed(encoder_shared))])\n",
    "    \n",
    "    for i in range(1, block_shared_n+1):\n",
    "        decoder_shared['block_shared_res'+str(i+1)] = create_res_block(decoder_shared[next(reversed(decoder_shared))])\n",
    "        \n",
    "    return decoder_shared#[next(reversed(decoder_shared))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generator(X, layer_n, res_block_n):\n",
    "    \n",
    "    num_filters = X.get_shape().as_list()[-1]\n",
    "    \n",
    "    decoder = OrderedDict()\n",
    "    \n",
    "    decoder['block_res1'] = create_res_block(X, num_filters=num_filters)\n",
    "        \n",
    "    for i in range(1, res_block_n):\n",
    "        decoder['block_res'+str(i+1)] = create_res_block(decoder[next(reversed(decoder))],\n",
    "                                                           num_filters=num_filters)\n",
    "\n",
    "    for i in range(0, layer_n-1):\n",
    "        \n",
    "        decoder['layer_deconv'+str(i+1)] = create_conv_layer(decoder[next(reversed(decoder))],\n",
    "                                       num_filters=num_filters//2,\n",
    "                                       filter_size=2,\n",
    "                                       strides=[1,2,2,1],\n",
    "                                       deconv=True,\n",
    "                                       batch_normalization=True,\n",
    "                                       activation='relu')\n",
    "        \n",
    "        num_filters = num_filters//2\n",
    "    \n",
    "    decoder['layer_deconv_final'] = create_conv_layer(decoder[next(reversed(decoder))],\n",
    "                                       num_filters=num_filters,\n",
    "                                       filter_size=1,\n",
    "                                       strides=[1,1,1,1],\n",
    "                                       deconv=True,\n",
    "                                       activation='tanh')\n",
    "    \n",
    "    return decoder#[next(reversed(decoder))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_discrimiator(X, num_filters, layer_n):\n",
    "    \n",
    "    discriminator = OrderedDict()\n",
    "    \n",
    "    discriminator['layer_discrim_conv1'] = create_conv_layer(input_layer=X,\n",
    "                                       num_filters=num_filters,\n",
    "                                       filter_size=3,\n",
    "                                       strides=[1,2,2,1])\n",
    "    \n",
    "    for i in range(1, layer_n):\n",
    "        \n",
    "        num_filters = num_filters * 2\n",
    "        \n",
    "        discriminator['layer_discrim_conv'+str(i+1)] = create_conv_layer(discriminator[next(reversed(discriminator))],\n",
    "                                       num_filters=num_filters,\n",
    "                                       filter_size=3,\n",
    "                                       strides=[1,2,2,1])\n",
    "    \n",
    "    return discriminator#[next(reversed(discriminator))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_encoding_loss(x):\n",
    "    x_2 = tf.pow(x, 2)\n",
    "    encoding_loss = tf.reduce_mean(x_2)\n",
    "    return encoding_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_l1_loss(x,y):\n",
    "    return tf.reduce_mean(tf.subtract(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_true_acc(predictions):\n",
    "    predictions = tf.greater_equal(predictions, 0.5)\n",
    "    acc = tf.reduce_sum(tf.equal(predictions, 1.0)) / (1.0 * tf.size(predictions))\n",
    "    return acc\n",
    "\n",
    "def _compute_fake_acc(predictions):\n",
    "    predictions = tf.less(predictions, 0.5)\n",
    "    acc = tf.reduce_sum(tf.equal(predictions, 1.0)) / (1.0 * tf.size(predictions))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build Graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X_A = tf.placeholder(tf.float32, shape=[1, None, None, 3], name='X_A')\n",
    "X_B = tf.placeholder(tf.float32, shape=[1, None, None, 3], name='X_B')\n",
    "\n",
    "encode_A = create_encoder(X_A, layer_n=3, res_block_n=3)\n",
    "encode_B = create_encoder(X_B, layer_n=3, res_block_n=3)\n",
    "\n",
    "encode_AB = tf.concat([encode_A[next(reversed(encode_A))], encode_B[next(reversed(encode_B))]], axis=1)\n",
    "\n",
    "shared_in = tf.placeholder(tf.float32, shape=encode_AB.get_shape().as_list(), name='shared_in')\n",
    "shared_ly = create_shared_layers(shared_in, block_shared_n=3)\n",
    "\n",
    "decode_ch = shared_ly[next(reversed(shared_ly))].get_shape().as_list()[-1]\n",
    "\n",
    "decode_A_in = tf.placeholder(tf.float32, shape=[1, None, None, decode_ch], name='decode_A_in')\n",
    "decode_A = create_generator(decode_A_in, layer_n=3, res_block_n=3)\n",
    "\n",
    "decode_B_in = tf.placeholder(tf.float32, shape=[1, None, None, decode_ch], name='decode_B_in')\n",
    "decode_B = create_generator(decode_B_in, layer_n=3, res_block_n=3)\n",
    "\n",
    "X_aa, X_ba = tf.split(decode_A[next(reversed(decode_A))], \n",
    "                      num_or_size_splits=2, axis=1)\n",
    "X_ab, X_bb = tf.split(decode_B[next(reversed(decode_B))], \n",
    "                      num_or_size_splits=2, axis=1)\n",
    "\n",
    "#dis\n",
    "outs_a = create_discrimiator(tf.placeholder(tf.float32, shape=[1, None, None, 3], name='X_ba'), \n",
    "                             num_filters=3, layer_n=6)\n",
    "outs_b = create_discrimiator(tf.placeholder(tf.float32, shape=[1, None, None, 3], name='X_ab')\n",
    "                             ,num_filters=3, layer_n=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_update(sess, images_a, images_b, hyperparameters):\n",
    "    \n",
    "    encode_ab = sess.run([encode_AB], \n",
    "                                 feed_dict={X_A:images_a, \n",
    "                                            X_B:images_b})\n",
    "    shared = sess.run([shared_ly[next(reversed(shared_ly))]], \n",
    "                                 feed_dict={shared_in:encode_ab[0]})\n",
    "    \n",
    "    x_aa, x_ba, x_ab, x_bb = sess.run([X_aa, X_ba, X_ab, X_bb], \n",
    "                                               feed_dict={decode_A_in:shared[0],\n",
    "                                                          decode_B_in:shared[0]})\n",
    "\n",
    "    data_a = tf.concat([images_a, x_ba], axis=1)\n",
    "    data_b = tf.concat([images_b, x_ab], axis=1)\n",
    "    \n",
    "    \n",
    "    out_a, out_b = sess.run([outs_a[next(reversed(outs_a))], outs_b[next(reversed(outs_b))]],\n",
    "                            feed_dict={x_ba:data_a, x_ab:data_b})\n",
    "        \n",
    "\n",
    "    out_true_a, out_fake_a = tf.contrib.layers.flatten(tf.split(out_a, \n",
    "                                      num_or_size_splits=[tf.shape(image_a)[1],\n",
    "                                                          tf.shape(x_ba)[1]], \n",
    "                                      axis=1))\n",
    "    \n",
    "    out_true_b, out_fake_b = tf.flatten(tf.split(out_b, \n",
    "                                      num_or_size_splits=[tf.shape(image_b)[1],\n",
    "                                                          tf.shape(x_ab)[1]], \n",
    "                                      axis=1))\n",
    "    \n",
    "    out_true_n = tf.size(out_true_a)\n",
    "    out_fake_n = tf.size(out_fake_a)\n",
    "    \n",
    "    all1 = tf.Variable(tf.constant(1.0, shape=[out_true_n]))\n",
    "    all0 = tf.Variable(tf.constant(0.0, shape=[out_fake_n]))\n",
    "    \n",
    "    ad_true_loss_a = tf.nn.sigmoid_cross_entropy_with_logits(out_true_a, all1)\n",
    "    ad_true_loss_b = tf.nn.sigmoid_cross_entropy_with_logits(out_true_b, all1)\n",
    "    ad_fake_loss_a = tf.nn.sigmoid_cross_entropy_with_logits(out_fake_a, all0)\n",
    "    ad_fake_loss_b = tf.nn.sigmoid_cross_entropy_with_logits(out_fake_b, all0)\n",
    "    \n",
    "    ad_loss_a = ad_true_loss_a + ad_fake_loss_a\n",
    "    ad_loss_b = ad_true_loss_b + ad_fake_loss_b\n",
    "    \n",
    "    true_a_acc = _compute_true_acc(out_true_a)\n",
    "    true_b_acc = _compute_true_acc(out_true_b)\n",
    "    fake_a_acc = _compute_fake_acc(out_fake_a)\n",
    "    fake_b_acc = _compute_fake_acc(out_fake_b)\n",
    "    \n",
    "    print(\"True Acc:\",0.5*(true_a_acc+true_b_acc),\" Fake Acc:\",0.5*(fake_a_acc+fake_b_acc))\n",
    "                       \n",
    "    cost = hyperparameters['gan_w'] * ( ad_loss_a + ad_loss_b )\n",
    "                       \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.0001,beta1=0.5,beta2=0.999).minimize(cost)\n",
    "                       \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_update(sess, images_a, images_b, hyperparameters):\n",
    "    \n",
    "    encode_ab = sess.run([encode_AB], \n",
    "                                 feed_dict={X_A:images_a, \n",
    "                                            X_B:images_b})\n",
    "    shared = sess.run([shared_ly[next(reversed(shared_ly))]], \n",
    "                                 feed_dict={shared_in:encode_ab[0]})\n",
    "    \n",
    "    x_aa, x_ba, x_ab, x_bb = sess.run([X_aa, X_ba, X_ab, X_bb], \n",
    "                                               feed_dict={decode_A_in:shared[0],\n",
    "                                                          decode_B_in:shared[0]})\n",
    "    #a2b\n",
    "    x_bab = sess.run([encode_A[next(reversed(encode_A))]], feed_dict={X_A:x_ba})\n",
    "    shared_bab = sess.run([shared_ly[next(reversed(shared))]], feed_dict={shared_in:x_bab})\n",
    "    x_bab = sess.run([decode_B[next(reversed(decode_B))]], feed_dict={decode_B_in:shared_bab})\n",
    "    \n",
    "    #b2a\n",
    "    x_aba = sess.run([encode_B[next(reversed(encode_B))]], feed_dict={X_B:x_ab})\n",
    "    shared_aba = sess.run([shared_ly[next(reversed(shared))]], feed_dict={shared_in:x_aba})\n",
    "    x_aba = sess.run([decode_A[next(reversed(decode_A))]], feed_dict={decode_A_in:shared_aba})\n",
    "    \n",
    "    #dis\n",
    "    out_a, out_b = sess.run([outs_a[next(reversed(outs_a))], outs_b[next(reversed(outs_b))]],\n",
    "                            feed_dict={X_ba:x_ba, X_ab:x_ab})\n",
    "    \n",
    "    outs_a = tf.contrib.layers.flatten(out_a)\n",
    "    outs_b = tf.contrib.layers.flatten(out_b)\n",
    "    \n",
    "    #loss\n",
    "    all_ones = tf.Variable(tf.constant(1.0, shape=[tf.size(outs_a)]))\n",
    "    \n",
    "    ad_loss_a = tf.nn.sigmoid_cross_entropy_with_logits(logits=outs_a, labels=all_ones)\n",
    "    ad_loss_b = tf.nn.sigmoid_cross_entropy_with_logits(logits=outs_b, labels=all_ones)\n",
    "    \n",
    "    enc_loss  = compute_encoding_loss(shared)\n",
    "    enc_bab_loss = compute_encoding_loss(shared_bab)\n",
    "    enc_aba_loss = compute_encoding_loss(shared_aba)\n",
    "    ll_loss_a = compute_l1_loss(x_aa, x_A)\n",
    "    ll_loss_b = compute_l1_loss(x_bb, x_B)\n",
    "    ll_loss_aba = compute_l1_loss(x_aba, x_A)\n",
    "    ll_loss_bab = compute_l1_loss(x_bab, x_B)\n",
    "    \n",
    "    total_loss = hyperparameters['gan_w'] * (ad_loss_a + ad_loss_b) + \\\n",
    "                 hyperparameters['ll_direct_link_w'] * (ll_loss_a + ll_loss_b) + \\\n",
    "                 hyperparameters['ll_cycle_link_w'] * (ll_loss_aba + ll_loss_bab) + \\\n",
    "                 hyperparameters['kl_direct_link_w'] * (enc_loss + enc_loss) + \\\n",
    "                 hyperparameters['kl_cycle_link_w'] * (enc_bab_loss + enc_aba_loss)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.0001, beta1=0.5,beta2=0.999).minimize(total_loss)\n",
    "                       \n",
    "    return x_aa, x_ba, x_ab, x_bb, x_aba, x_bab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epoch = 100\n",
    "hyperparameters = {}\n",
    "\n",
    "hyperparameters['gan_w'] = 10\n",
    "hyperparameters['ll_direct_link_w'] = 100\n",
    "hyperparameters['ll_cycle_link_w'] = 100\n",
    "hyperparameters['kl_direct_link_w'] = 0.1\n",
    "hyperparameters['kl_cycle_link_w'] = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir = \"/home/praveen/Downloads/ezgif-4-57d76e4278-gif-jpg\"\n",
    "\n",
    "images_names = os.listdir(images_dir)\n",
    "\n",
    "img_names = []    \n",
    "\n",
    "for file in images_names:\n",
    "    img_name = os.path.join(images_dir, file)\n",
    "    img_names.append(img_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(484, 1284, 3)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imread(img_name).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Dimension 0 in both shapes must be equal, but are 642 and 644 for 'concat_4' (op: 'ConcatV2') with input shapes: [1,484,642,3], [1,484,644,3], [] and with computed input tensors: input[2] = <1>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    685\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[1;32m    687\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimension 0 in both shapes must be equal, but are 642 and 644 for 'concat_4' (op: 'ConcatV2') with input shapes: [1,484,642,3], [1,484,644,3], [] and with computed input tensors: input[2] = <1>.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-142-0f6342cdfe9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mimages_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m642\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m484\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m642\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                                      \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m642\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m484\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m642\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mdiscriminator_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mgenerator_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#             en = sess.run([encode_A],feed_dict={X_A:images_a})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-132-1365f8f20cd2>\u001b[0m in \u001b[0;36mdiscriminator_update\u001b[0;34m(sess, images_a, images_b, hyperparameters)\u001b[0m\n\u001b[1;32m     11\u001b[0m                                                           decode_B_in:shared[0]})\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mdata_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimages_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_ba\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mdata_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimages_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_ab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1097\u001b[0m               tensor_shape.scalar())\n\u001b[1;32m   1098\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_concat_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36m_concat_v2\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m    704\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_graph_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m--> 706\u001b[0;31m         \"ConcatV2\", values=values, axis=axis, name=name)\n\u001b[0m\u001b[1;32m    707\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2956\u001b[0m         op_def=op_def)\n\u001b[1;32m   2957\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2958\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2959\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2960\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2207\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2209\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2210\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2211\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2158\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2159\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2161\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, require_shape_fn)\u001b[0m\n\u001b[1;32m    625\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    626\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m                                   require_shape_fn)\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    689\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimension 0 in both shapes must be equal, but are 642 and 644 for 'concat_4' (op: 'ConcatV2') with input shapes: [1,484,642,3], [1,484,644,3], [] and with computed input tensors: input[2] = <1>."
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for ep in range(1):\n",
    "        print(ep)\n",
    "        for img_name in img_names[0:1]:\n",
    "            img = cv2.imread(img_name)/255.\n",
    "            images_a, images_b = np.reshape(np.array(img[:,:642,:], dtype=\"float32\"),(1,484, 642, 3)),  \\\n",
    "                                    np.reshape(np.array(img[:,642:,:], dtype=\"float32\"),(1,484, 642, 3))\n",
    "        \n",
    "            discriminator_update(sess, images_a, images_b, hyperparameters)\n",
    "            generator_update(sess, images_a, images_b, hyperparameters)\n",
    "#             en = sess.run([encode_A],feed_dict={X_A:images_a})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-131-cedd7ac36334>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
