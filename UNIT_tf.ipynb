{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Image-image Translation(UNIT) TensorFlow Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/praveen/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funciton to chose the activation function\n",
    "def activate(linear, activation='leaky_relu'):\n",
    "        if activation == 'sigmoid':\n",
    "            return tf.nn.sigmoid(linear)\n",
    "        elif activation == 'softmax':\n",
    "            return tf.nn.softmax(linear)\n",
    "        elif activation == 'tanh':\n",
    "            return tf.nn.tanh(linear)\n",
    "        elif activation == 'relu':\n",
    "            return tf.nn.relu(linear)\n",
    "        elif activation == 'leaky_relu':\n",
    "            return tf.nn.leaky_relu(linear)\n",
    "        elif activation == 'linear':\n",
    "            return linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_noise_layer(input_layer, std=1.0):\n",
    "    noise = tf.random_normal(shape=tf.shape(input_layer), mean=0.0, stddev=std, dtype=tf.float32) \n",
    "    return tf.add(input_layer, noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_normalization_layer(input_layer):\n",
    "    \n",
    "    dimension = input_layer.get_shape().as_list()[-1]\n",
    "    \n",
    "    mean, variance = tf.nn.moments(input_layer, axes=[0, 1, 2])\n",
    "    \n",
    "    beta = tf.Variable(tf.constant(0.0, shape=[dimension]))\n",
    "    \n",
    "    gamma = tf.Variable(tf.constant(1.0, shape=[dimension]))\n",
    "\n",
    "    bn_layer = tf.nn.batch_normalization(input_layer, mean, variance, beta, gamma, variance_epsilon=1e-10)\n",
    "\n",
    "    return bn_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conv_layer(input_layer,     # The previous layer.\n",
    "                   num_input_channels, # Num. channels in prev. layer.\n",
    "                   filter_size,        # Width and height of each filter.\n",
    "                   num_filters,        # Number of filters.\n",
    "                   use_pooling=False,  \n",
    "                   pad = [],\n",
    "                   strides=[1,1,1,1],\n",
    "                   deconv=False,\n",
    "                   out_shape = [],      # Output shape in case of deconv \n",
    "                   batch_normalization=False,\n",
    "                   activation='leaky_relu'): # Use 2x2 max-pooling.\n",
    "\n",
    "    # Shape of the filter-weights for the convolution. \n",
    "    # This format is determined by the TensorFlow API.\n",
    "    if deconv:\n",
    "        shape = [filter_size, filter_size, num_filters, num_input_channels]\n",
    "    else:\n",
    "        shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
    "\n",
    "    # Create new weights aka. filters with the given shape.\n",
    "    weights = tf.Variable(tf.truncated_normal(shape, stddev=0.05))\n",
    "\n",
    "    # Create new biases, one for each filter.\n",
    "    biases = tf.Variable(tf.constant(1.0, shape=[num_filters]))\n",
    "\n",
    "    if len(pad) > 0:\n",
    "        input_layer = tf.pad(input_layer, [[0, 0], [pad[0], pad[0]], [pad[1], pad[1]], [0, 0]], \"CONSTANT\")\n",
    "    \n",
    "    # Create the TensorFlow operation for de-convolution.\n",
    "    if deconv:\n",
    "        #in_shape = input_layer.get_shape().as_list()\n",
    "        in_shape = tf.shape(input_layer)\n",
    "    \n",
    "        out_h = ((in_shape[1] - 1) * strides[1]) + filter_size - 2 * pad[0]\n",
    "        \n",
    "        out_w = ((in_shape[2] - 1) * strides[2]) + filter_size - 2 * pad[1]\n",
    "        \n",
    "        output_shape = tf.stack([in_shape[0], out_h, out_w, num_filters])\n",
    "        \n",
    "        layer = tf.nn.conv2d_transpose(value=input_layer, \n",
    "                                       filter=weights, \n",
    "                                       output_shape=output_shape, \n",
    "                                       strides=strides, \n",
    "                                       padding='VALID')\n",
    "        \n",
    "    else:\n",
    "        layer = tf.nn.conv2d(input=input_layer,\n",
    "                         filter=weights,\n",
    "                         strides=strides,\n",
    "                         padding='VALID')\n",
    "\n",
    "    # Add the biases to the results of the convolution.\n",
    "    # A bias-value is added to each filter-channel.\n",
    "    layer = tf.add(layer,biases)\n",
    "\n",
    "    # Use pooling to down-sample the image resolution?\n",
    "    if use_pooling:\n",
    "        # This is 2x2 max-pooling, which means that we\n",
    "        # consider 2x2 windows and select the largest value\n",
    "        # in each window. Then we move 2 pixels to the next window.\n",
    "        layer = tf.nn.max_pool(value=layer,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME')\n",
    "\n",
    "    # Batch normalization\n",
    "    if batch_normalization:\n",
    "        layer = batch_normalization_layer(layer)\n",
    "    \n",
    "    # Activation of the layers (ReLU).\n",
    "    layer = activate(layer, activation=activation)\n",
    "\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_res_block(X, num_input_channels=3, filter_size=3, num_filters=3, pad=[1,1]):\n",
    "    \n",
    "    layer_conv1 = create_conv_layer(input_layer=X,\n",
    "                                       num_input_channels=num_input_channels,\n",
    "                                       filter_size=filter_size,\n",
    "                                       num_filters=num_filters,\n",
    "                                       pad=pad,\n",
    "                                       use_pooling=False,\n",
    "                                       batch_normalization=True,\n",
    "                                       activation='relu')\n",
    "    \n",
    "    layer_conv2 = create_conv_layer(input_layer=layer_conv1,\n",
    "                                       num_input_channels=num_input_channels,\n",
    "                                       filter_size=filter_size,\n",
    "                                       num_filters=num_filters,\n",
    "                                       pad=pad,\n",
    "                                       use_pooling=False,\n",
    "                                       batch_normalization=False,\n",
    "                                       activation='linear')\n",
    "    \n",
    "    layer_conv2 = batch_normalization_layer(layer_conv2)\n",
    "    \n",
    "    layer_conv2 += X\n",
    "    \n",
    "    layer_res = activate(layer_conv2, activation='relu')\n",
    "    \n",
    "    return layer_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoders(X, num_input_channels, layer_n, res_block_n):\n",
    "    \n",
    "    encoder_A = OrderedDict()\n",
    "    encoder_B = OrderedDict()\n",
    "    \n",
    "    encoder_A['layer_conv1'] = create_conv_layer(input_layer=X,\n",
    "                                       num_input_channels=num_input_channels,\n",
    "                                       num_filters=num_input_channels,\n",
    "                                       filter_size=7,\n",
    "                                       strides=[1,2,2,1],\n",
    "                                       pad=[3,3])\n",
    "    \n",
    "    encoder_B['layer_conv1'] = create_conv_layer(input_layer=X,\n",
    "                                       num_input_channels=num_input_channels,\n",
    "                                       num_filters=num_input_channels,\n",
    "                                       filter_size=7,\n",
    "                                       strides=[1,2,2,1],\n",
    "                                       pad=[3,3])\n",
    "    \n",
    "    for i in range(1, layer_n):\n",
    "        encoder_A['layer_conv'+str(i+1)] = create_conv_layer(encoder_A[next(reversed(encoder_A))],\n",
    "                                       num_input_channels=num_input_channels,\n",
    "                                       num_filters=num_input_channels*2,\n",
    "                                       filter_size=3,\n",
    "                                       strides=[1,2,2,1],\n",
    "                                       pad=[1,1])\n",
    "        \n",
    "        encoder_B['layer_conv'+str(i+1)] = create_conv_layer(encoder_B[next(reversed(encoder_B))],\n",
    "                                       num_input_channels=num_input_channels,\n",
    "                                       num_filters=num_input_channels*2,\n",
    "                                       filter_size=3,\n",
    "                                       strides=[1,2,2,1],\n",
    "                                       pad=[1,1])\n",
    "        \n",
    "        num_input_channels = num_input_channels*2\n",
    "        \n",
    "    for i in range(1, res_block_n):\n",
    "        encoder_A['block_en_res'+str(i+1)] = create_res_block(encoder_A[next(reversed(encoder_A))],\n",
    "                                                           num_input_channels=num_input_channels,\n",
    "                                                           num_filters=num_input_channels)\n",
    "        \n",
    "        encoder_B['block_en_res'+str(i+1)] = create_res_block(encoder_B[next(reversed(encoder_B))],\n",
    "                                                           num_input_channels=num_input_channels,\n",
    "                                                           num_filters=num_input_channels)\n",
    "        \n",
    "    return encoder_A, encoder_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_shared_layers(X, block_shared_n):\n",
    "    encoder_shared = OrderedDict()\n",
    "    decoder_shared = OrderedDict()\n",
    "    \n",
    "    encoder_shared_ = create_res_block(X)\n",
    "    encoder_shared['block_shared_res1'] = gaussian_noise_layer(encoder_shared_)\n",
    "    \n",
    "    for i in range(1, block_shared_n+1):\n",
    "        encoder_shared_ = create_res_block(encoder_shared['block_shared_res'+str(i)])\n",
    "        encoder_shared['block_shared_res'+str(i+1)] = gaussian_noise_layer(encoder_shared_)\n",
    "    \n",
    "    decoder_shared['block_shared_res1'] = create_res_block(encoder_shared['block_shared_res'+str(block_shared_n)])\n",
    "    \n",
    "    for i in range(1, block_shared_n+1):\n",
    "        decoder_shared['block_shared_res'+str(i+1)] = create_res_block(decoder_shared['block_shared_res'+str(i)])\n",
    "        \n",
    "    return decoder_shared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generator(X, num_input_channels, layer_n, res_block_n, num_output_channels):\n",
    "\n",
    "    decoder_A = OrderedDict()\n",
    "    decoder_B = OrderedDict()\n",
    "    \n",
    "    decoder_A['block_res1'] = create_res_block(X,\n",
    "                                               num_input_channels=num_input_channels,\n",
    "                                               num_filters=num_input_channels)\n",
    "        \n",
    "    decoder_B['block_res1'] = create_res_block(X,\n",
    "                                               num_input_channels=num_input_channels,\n",
    "                                               num_filters=num_input_channels)\n",
    "        \n",
    "    for i in range(1, res_block_n):\n",
    "        decoder_A['block_res'+str(i+1)] = create_res_block(decoder_A[next(reversed(decoder_A))],\n",
    "                                                           num_input_channels=num_input_channels,\n",
    "                                                           num_filters=num_input_channels)\n",
    "        \n",
    "        decoder_B['block_res'+str(i+1)] = create_res_block(decoder_B[next(reversed(decoder_B))],\n",
    "                                                           num_input_channels=num_input_channels,\n",
    "                                                           num_filters=num_input_channels)\n",
    "    for i in range(0, layer_n):\n",
    "        \n",
    "        decoder_A['layer_deconv'+str(i+1)] = create_conv_layer(decoder_A[next(reversed(decoder_A))],\n",
    "                                       num_input_channels=num_input_channels,\n",
    "                                       num_filters=num_input_channels//2,\n",
    "                                       filter_size=3,\n",
    "                                       strides=[1,2,2,1],\n",
    "                                       pad=[1,1],\n",
    "                                       deconv=True,\n",
    "                                       batch_normalization=True,\n",
    "                                       activation='relu')\n",
    "        \n",
    "        decoder_B['layer_deconv'+str(i+1)] = create_conv_layer(decoder_B[next(reversed(decoder_B))],\n",
    "                                       num_input_channels=num_input_channels,\n",
    "                                       num_filters=num_input_channels//2,\n",
    "                                       filter_size=3,\n",
    "                                       strides=[1,2,2,1],\n",
    "                                       pad=[1,1],\n",
    "                                       deconv=True,\n",
    "                                       batch_normalization=True,\n",
    "                                       activation='relu')\n",
    "        \n",
    "        num_input_channels = num_input_channels//2\n",
    "    \n",
    "    decoder_A['layer_deconv_final'] = create_conv_layer(decoder_A[next(reversed(decoder_A))],\n",
    "                                       num_input_channels=num_input_channels,\n",
    "                                       num_filters=num_output_channels,\n",
    "                                       filter_size=1,\n",
    "                                       strides=[1,1,1,1],\n",
    "                                       pad=[0,0],\n",
    "                                       activation='tanh')\n",
    "    \n",
    "    decoder_B['layer_deconv_final'] = create_conv_layer(decoder_B[next(reversed(decoder_B))],\n",
    "                                       num_input_channels=num_input_channels,\n",
    "                                       num_filters=num_output_channels,\n",
    "                                       filter_size=1,\n",
    "                                       strides=[1,1,1,1],\n",
    "                                       pad=[0,0],\n",
    "                                       activation='tanh')\n",
    "    \n",
    "    return decoder_A, decoder_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_discrimiator(X_A, X_B, num_input_channels, num_filters, layer_n):\n",
    "    discrim_A = OrderedDict()\n",
    "    discrim_B = OrderedDict()\n",
    "    \n",
    "    discrim_A['layer_discrim_conv1'] = create_conv_layer(input_layer=X_A,\n",
    "                                       num_input_channels=num_input_channels,\n",
    "                                       num_filters=num_filters,\n",
    "                                       filter_size=3,\n",
    "                                       strides=[1,2,2,1],\n",
    "                                       pad=[1,1])\n",
    "    discrim_B['layer_discrim_conv1'] = create_conv_layer(input_layer=X_B,\n",
    "                                       num_input_channels=num_input_channels,\n",
    "                                       num_filters=num_filters,\n",
    "                                       filter_size=3,\n",
    "                                       strides=[1,2,2,1],\n",
    "                                       pad=[1,1])\n",
    "    \n",
    "    num_input_channels = num_filters\n",
    "    \n",
    "    for i in range(1, layer_n):\n",
    "        \n",
    "        discrim_A['layer_discrim_conv'+str(i+1)] = create_conv_layer(discrim_A[next(reversed(discrim_A))],\n",
    "                                       num_input_channels=num_input_channels,\n",
    "                                       num_filters=num_filters*2,\n",
    "                                       filter_size=3,\n",
    "                                       strides=[1,2,2,1],\n",
    "                                       pad=[1,1])\n",
    "        discrim_B['layer_discrim_conv'+str(i+1)] = create_conv_layer(discrim_B[next(reversed(discrim_B))],\n",
    "                                           num_input_channels=num_input_channels,\n",
    "                                           num_filters=num_filters*2,\n",
    "                                           filter_size=3,\n",
    "                                           strides=[1,2,2,1],\n",
    "                                           pad=[1,1])\n",
    "        \n",
    "        num_input_channels = num_input_channels * 2\n",
    "    \n",
    "        decoder_A['layer_deconv_final'] = create_conv_layer(decoder_A[next(reversed(decoder_A))],\n",
    "                                       num_input_channels=num_input_channels,\n",
    "                                       num_filters=num_output_channels,\n",
    "                                       filter_size=1,\n",
    "                                       strides=[1,1,1,1],\n",
    "                                       pad=[0,0],\n",
    "                                       activation='tanh')\n",
    "    \n",
    "    decoder_B['layer_deconv_final'] = create_conv_layer(decoder_B[next(reversed(decoder_B))],\n",
    "                                       num_input_channels=num_input_channels,\n",
    "                                       num_filters=num_output_channels,\n",
    "                                       filter_size=1,\n",
    "                                       strides=[1,1,1,1],\n",
    "                                       pad=[0,0],\n",
    "                                       activation='tanh')\n",
    "    \n",
    "    return discrim_A, discrim_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(OrderedDict([('layer_discrim_conv1',\n",
       "               <tf.Tensor 'LeakyRelu/Maximum:0' shape=(?, 128, 128, 3) dtype=float32>),\n",
       "              ('layer_discrim_conv2',\n",
       "               <tf.Tensor 'LeakyRelu_2/Maximum:0' shape=(?, 64, 64, 6) dtype=float32>),\n",
       "              ('layer_discrim_conv3',\n",
       "               <tf.Tensor 'LeakyRelu_4/Maximum:0' shape=(?, 32, 32, 6) dtype=float32>)]),\n",
       " OrderedDict([('layer_discrim_conv1',\n",
       "               <tf.Tensor 'LeakyRelu_1/Maximum:0' shape=(?, 128, 128, 3) dtype=float32>),\n",
       "              ('layer_discrim_conv2',\n",
       "               <tf.Tensor 'LeakyRelu_3/Maximum:0' shape=(?, 64, 64, 6) dtype=float32>),\n",
       "              ('layer_discrim_conv3',\n",
       "               <tf.Tensor 'LeakyRelu_5/Maximum:0' shape=(?, 32, 32, 6) dtype=float32>)]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "create_discrimiator(tf.placeholder(tf.float32, shape=[None, 256, 256, 3], name='x'),tf.placeholder(tf.float32, shape=[None, 256, 256, 3], name='x'),3, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
